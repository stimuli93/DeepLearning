## Fundamentals of DeepLearning

*layers.py* consists of forward & backward pass for the following layers:
* dense layer (fully-connected layer)
* sigmoid layer
* tanh layer
* relu layer
* dropout layer
* softmax_loss layer

*rnn_layers.py* consists of forward & backward pass of:
* vanilla rnn
* lstm
* embedding layer

*gradient_check_layers.py* consists of gradient checking of the above mentioned layers

*optimizers.py* consists of implementation of following optimizers:
* SGD
* Momentum
* Adagrad
* Rmsprop
* Adam

*initializations.py* consists of following initializations schemes:
* uniform initialization
* xavier initialization

*Activation Analysis.ipynb* is an iPython notebook which performs Analysis of Gradient at hidden layers of Relu,
Sigmoid and Tanh Activations to study the vanishing gradient problem
